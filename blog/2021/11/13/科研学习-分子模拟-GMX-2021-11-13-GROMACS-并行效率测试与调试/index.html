<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>GROMACS 并行效率测试与调试 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="版权所有：GROMACS (2019.3 GPU版) 并行效率测试及调试思路 - 分子模拟 (Molecular Modeling) - 计算化学公社 (keinsci.com) 补充文章：物美价廉：GROMACS 2018在GPU节点上的使用|Jerkwin GROMACS的并行相比Gaussian等量化软件要复杂的多。GMX手册上有一章Getting good performance from">
<meta property="og:type" content="article">
<meta property="og:title" content="GROMACS 并行效率测试与调试">
<meta property="og:url" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="版权所有：GROMACS (2019.3 GPU版) 并行效率测试及调试思路 - 分子模拟 (Molecular Modeling) - 计算化学公社 (keinsci.com) 补充文章：物美价廉：GROMACS 2018在GPU节点上的使用|Jerkwin GROMACS的并行相比Gaussian等量化软件要复杂的多。GMX手册上有一章Getting good performance from">
<meta property="og:locale">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478601.php">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478612.php">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum.php">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478614.php">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478615.php">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367939437841.php">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478617.php">
<meta property="og:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478618.php">
<meta property="article:published_time" content="2021-11-13T08:27:13.000Z">
<meta property="article:modified_time" content="2021-11-13T09:11:55.476Z">
<meta property="article:author" content="FY-Han">
<meta property="article:tag" content="gmx">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478601.php">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Buscar"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Buscar"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://FY-Han.github.io.git"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-科研学习-分子模拟-GMX-2021-11-13-GROMACS-并行效率测试与调试" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/" class="article-date">
  <time class="dt-published" datetime="2021-11-13T08:27:13.000Z" itemprop="datePublished">2021-11-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      GROMACS 并行效率测试与调试
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>版权所有：<a target="_blank" rel="noopener" href="http://bbs.keinsci.com/thread-13861-1-4.html">GROMACS (2019.3 GPU版) 并行效率测试及调试思路 - 分子模拟 (Molecular Modeling) - 计算化学公社 (keinsci.com)</a></p>
<p>补充文章：<a target="_blank" rel="noopener" href="https://jerkwin.github.io/2019/08/28/物美价廉-GROMACS_2018在GPU节点上的使用/">物美价廉：GROMACS 2018在GPU节点上的使用|Jerkwin</a></p>
<p>GROMACS的并行相比Gaussian等量化软件要复杂的多。GMX手册上有一章<a target="_blank" rel="noopener" href="http://manual.gromacs.org/documentation/current/user-guide/mdrun-performance.html">Getting good performance from mdrun</a>，介绍了很多基本概念和例子。不过看完后还是一头雾水，不知道怎么才能获得最佳的运行效率。本文对这些信息进行整理和测试，给出一个基本的调试思路。<br>另外：<a target="_blank" rel="noopener" href="http://bbs.keinsci.com/thread-13910-1-1.html?tdsourcetag=s_pcqq_aiomsg">GROMACS GPU加速性能测试文章(JCC,2019)</a>，这篇文章为了主要目的是比较显卡的性价比，所有的测试都是aggregate performance，也就是所有的GMX任务都用1个Rank，一块显卡跑。如果有N个显卡，就跑N个任务，然后把总的ns/day数加起来。这样其实无从知道Gromacs的并行效率。不过这篇文章也说了：“On single-socket nodes with one GPU, using a single rank with as many OpenMP threads as available cores (or hardware threads) is usually fastest”。那么其他情况下gromacs的并行情况如何呢？<br>最后，这篇文章长而且复杂，<br><strong>没有耐心的同学可以直接看第四部分：结论——简单粗暴版，然后可以用第五部分的实用测试命令测试你将要跑的体系，选出最优条件，</strong><br><strong>需要购机的同学可以直接看第四部分：结论——给购机同学的建议。</strong><br><strong>使用超算中心同学可以直接看第四部分：结论——给使用超算中心同学的建议。</strong><br>有兴趣知道背后机制的同学可以详细阅读。</p>
<h2 id="先搞清楚几个概念。"><a href="#先搞清楚几个概念。" class="headerlink" title="先搞清楚几个概念。"></a><strong>先搞清楚几个概念。</strong></h2><h3 id="Rank-和-Thread"><a href="#Rank-和-Thread" class="headerlink" title="Rank 和 Thread"></a><strong>Rank 和 Thread</strong></h3><p>​    Rank大概可以翻译成进程，和Processes等价，Thread就线程。Gromacs可以在线程和进程这两个层面上并行，这两个的区别大家可以参看<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/25532384">线程和进程的区别是什么？</a>。一个Rank可以包含多个thread，在Gromacs并行时，如果使用Rank并行，会使用Domain Decompostion把体系切成小块，每块交给一个Rank去算，而在这个Rank中，多个Thread共同处理这一个小块。</p>
<h3 id="Gromacs的几种并行方式："><a href="#Gromacs的几种并行方式：" class="headerlink" title="Gromacs的几种并行方式："></a><strong>Gromacs的几种并行方式：</strong></h3><p><strong>1，外部的mpirun并行（rank级别并行）</strong><br>   需要安装openmpi，编译安装的时候加上-DGMX_MPI=on选项，编译产生的运行程序名默认是gmx_mpi，可以跨节点运行。实现方式是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mpirun -np 4 gmx_mpi mdrun</span><br></pre></td></tr></table></figure>
<p><strong>2，内部thread-mpi并行（rank级别并行）</strong><br>   Gromacs源码包含，编译时候默认支持，无法跨节点并行，无法和上面的mpirun并行同时使用，单节点运行时，比mpirun稍快，实现方式是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx mdrun -ntmpi 4</span><br></pre></td></tr></table></figure>
<p><strong>3，openmp并行（thread级别并行）</strong><br>   Gromacs源码包含，编译时候默认支持，无法跨节点并行，可以和上面两种MPI并行同时使用，实现方式是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx mdrun -ntomp</span><br></pre></td></tr></table></figure>
<p><strong>总结：</strong>单节点运行时，通常采用2+3或者只用3的方式运行，运行方式是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx mdrun -ntmpi 4 -ntomp 6       #4个MPI rank, 每个rank 使用6个线程，运行时占用24个核</span><br></pre></td></tr></table></figure>
<p>跨节点运行时，通常采用1+3的方式运行，运行方式是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mpirun -np 2 gmx_mpi mdrun -ntomp 6</span><br></pre></td></tr></table></figure>
<p>（2个MPI rank, 每个rank 使用6个线程，运行时占用12个核）<strong><em>这里需要注意的是，不同的并行方式下，GMX对显卡的默认利用方式会不一样，从而带来效率的巨大改变，参见下面的d)-4部分\</em></strong></p>
<h3 id="Gromacs中主要耗时的两类任务"><a href="#Gromacs中主要耗时的两类任务" class="headerlink" title="Gromacs中主要耗时的两类任务"></a><strong>Gromacs中主要耗时的两类任务</strong></h3><p><strong>1，粒子-粒子相互作用（particle-particle, PP）</strong><br>   在实空间中计算原子之间的两两相互作用力，这些作用主要包括两部分：<strong>非键原子间短程相互作用（NB）</strong>，以及<strong>成键原子之间的相互作用（BF）</strong>。在并行计算时，对于这部分任务采用<strong>Domain Decomposition</strong>的方法，也就是把整个胞像切蛋糕一样切成小块，每块交给一个Rank，在这个Rank中，几个Thread共同计算这个小块中的PP作用力。这里需要注意的的是，如果你的体系本身就比较小，如果使用的Rank太多，或者不合适，会出现no domain decompostion compatible with the given box 的错误。<br><strong>2，粒子网格埃瓦尔德（particle-mesh Ewald，PME)</strong><br>在倒空间中<strong>使用FFT对长程作用力进行计算</strong>，这里的“长程”和上面的“短程”是由mdp参数中的rcoulomb和rvdw参数决定的。这部分计算在倒空间中进行，不能像实空间中的PP那样，可以切成小块，因此Rank并行效率较低。所以如果总Rank数较少，那么这些Rank可以既做PP任务又做PME任务，随着Rank数的增多，更为合适的做法是分出少量的Rank专门做PME任务（当Rank数超过16之后，Gromacs会自动分配PME Rank，在此之前，可以用-npme 选项手动分配PME Rank数），<strong>如果用显卡算PME，目前最多只能分配一个Rank（一块显卡）来做PME计算。</strong></p>
<p><strong>d) Gromacs的显卡加速</strong><br><strong>1，显卡的并行级别</strong><br>一个Rank进程不能使用多个显卡，但是多个Rank进程可以使用一个显卡。因此如果电脑上有3块显卡，那MPI进程数(thread-mpi,或者openmpi都可以）起码要到3才能使用这3块显卡。<br><strong>2，显卡的加速内容</strong></p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478601.php" alt="img" style="zoom:50%;"></p>
<p>此图修改自Gromacs官网，显卡可以代替CPU计算图中的三部分(offloading)，分别是成键原子间的作用力<strong>(Bonded F，下文简称BF)</strong>，短程非键作用力(<strong>Non-bonded F，下文简称NB</strong>)，以及长程非键作用（<strong>PME，下文简称PME</strong>), 前两者也就是前面介绍的<strong>PP</strong>作用，其中Bonded 目前(2019.3版本)仅支持N卡，其中GPU加速的PME只支持单Rank计算并且mdp参数必须是PME-order=4，还有一些其他限制详见手册。<br><strong>3，**</strong>显卡的加速的开启以及查看方法<strong><br>这3个部分手动切换cpu/gpu在mdrun的选项中分别是-bonded -nb  -pme，
</strong>如果不清楚显卡执行了哪部分任务，可以在md.log中查看**，看到类似下面的描述</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Using 4 MPI threads                                                                        #使用四个MPI rank</span><br><span class="line">Using 10 OpenMP threads per tMPI thread                                                   #每个thread-MPI rank使用10个OpenMP thread</span><br><span class="line">On host node02 2 GPUs selected for this run.                                                #使用2个显卡计算</span><br><span class="line">Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:PP:0,PP:0,PP:1,PME:1                                                                     #2个PP任务分配给id&#x3D;0的显卡，1个PP任务和1个PME任务分配给id&#x3D;1的显卡</span><br><span class="line">PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU        #PP任务包括了计算大部分Bonded和短程Non-bonded作用</span><br><span class="line">PME tasks will do all aspects on the GPU                                                     #PME任务全部在GPU上进行</span><br></pre></td></tr></table></figure>
<p><strong>4，默认利用显卡加速情况</strong><br>这里单独强调一下默认情况下（就是不额外加控制显卡的参数）显卡会执行哪些任务：<br>单Rank并行下：<br>显卡自动执行NB+PME计算，BF交给GPU，如果使用-pme cpu选项强行把PME分配给CPU，那么显卡会自动执行NB+BF计算。<br>多rank并行下：<br>显卡会自动执行NB+BF计算，PME交给CPU，此时如果要利用显卡计算PME，必须使用-pme gpu -npme 1 单独分配一个rank来进行PME计算<br><strong>5，利用显卡加速的5种情况（以及对应的命令行选项）</strong><br>下面总结一下利用显卡加速的5种情况，<br>0：不用显卡加速（ -nb cpu -pme cpu -bonded cpu ）<br>1：用显卡做NB计算（-nb gpu -pme cpu -bonded cpu ）<br>2：用显卡做NB+PME计算（-nb gpu -pcme gpu -bonded cpu ）多rank并行还需加上 -npme 1<br>3：用显卡做NB+BF计算（-nb gpu -pme cpu -bonded gpu ）<br>4：用显卡做NB+PME+BF计算（-nb gpu -pme gpu -bonded gpu ）多rank并行还需加上 -npme 1<br>注意到这里我们<strong>只有用显卡加速了NB部分，才能进一步加速BF和PME部分</strong>。</p>
<h2 id="测试条件"><a href="#测试条件" class="headerlink" title="测试条件"></a><strong>测试条件</strong></h2><p><strong>a) 硬件环境</strong><br>cpu：XEON E5-2699V4 <em> 2，显卡：华硕TURBO-RTX2080-8G X 2 or 耕升GTX-1080 追风 X 2，内存：128G，主板：msi X10dai<br><strong>b) 软件环境</strong><br>操作系统：CentOS7，CUDA版本：10.2， gcc版本：7.3.1，gromacs版本：2019.3安装方法参考：<a target="_blank" rel="noopener" href="http://sobereva.com/457">GROMACS的安装方法</a><br><strong>c) 测试方法</strong><br>测试系统：<a target="_blank" rel="noopener" href="http://www.gromacs.org/GPU_acceleration">Gromacs官网</a>上的<a href="ftp://ftp.gromacs.org/pub/benchmarks/ADH_bench_systems.tar.gz">ADH例子</a>中的adh_cubic<br>体系大小：13.4W个原子，11</em>11*11nm的盒子<br>测试命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx mdrun -ntmpi 4 -ntomp 6 -s ADH.tpr -cpt 1440 -nsteps 15000 -resetstep 5000 -v -noconfout -gpu_id 0 -pme gpu -npme 1 -bonded gpu -nb gpu -gputasks 0001</span><br></pre></td></tr></table></figure>
<p>下面对用到的mdrun的一些选项进行做说明：<br>-ntmpi X         使用X个rank进行并行<br>-ntomp X        每个rank使用X个openmpi线程<br>-cpt  X          间隔X分钟写入checkpointfile<br>-nsteps X        一共跑X步md<br>-resetstep X      在第X步时开始重新计时（因为头几千步，gromacs会进行自动调试，此时速度还不稳定）<br>-gpu_id  01    使用id为X和Y的显卡计算，比如-gpu_id 0123，说明使用4块显卡进行计算, -gpu_id 12，使用第二块和第三款显卡。<br>-pme cpu/gpu   使用cpu或gpu进行pme计算<br>-nb cpu/gpu     使用cpu或gpu进行nb计算<br>-bonded cpu/gpu 使用cpu或gpu进行bonded force计算<br>-npme X         在多rank并行情况下，使用X个rank进行pme计算<br>-gputasks 0011  在多rank并行情况下，每个rank分配给哪块cpu，比如8 Rank并行时（-ntmpi 8），而我有3块gpu， 那么-gputasks 00001122 表明前4个Rank分给第一块显卡，中间两个Rank分给第二块显卡，最后2个Rank分给第三块显卡，这个选项不能和-gpu_id选项同时出现。至于这些Rank哪些是pme Rank 哪些是pp Rank则可以用-ddorder选项控制，详见手册中-ddorder选项的说明，默认的是pp Rank在前pme Rank 在后。</p>
<h2 id="测试内容"><a href="#测试内容" class="headerlink" title="测试内容"></a><strong>测试内容</strong></h2><p><strong>a) 单Rank，单显卡</strong><br>这时候可以调节的参数有openmp的核数-ntomp，以及是否把BF(Bonded F)，NB(Non-bonded F)，PME任务分配给显卡计算。</p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478612.php" alt="图一：计算速度随openmp线程数的变化情况，图中的5条曲线分别对应于把不同的任务分配给GPU时的情况" style="zoom:50%;"></p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum.php" alt="表一：从md.log中提取出的具体项目的耗时情况，第一行中的Tn表示线程数，红色加粗的是NB+BF部分的耗时，蓝色加粗的是PME部分的耗时。" style="zoom: 80%;"></p>
<p><strong>分析：</strong><br>1）BF(bonded force)部分的计算是否分配给GPU影响不大，显卡任务没饱和时，BF分给显卡较快，显卡任务饱和了以后BF分给CPU较快。<br>2）在拥有一个比较好的显卡情况下，把PME部分分配给显卡可以显著提高计算速度。<br>3）PME分配给显卡后，计算速度在10个openmp线程左右达到了上限，这是因为GPU的运算成为瓶颈。原因如下：对比表一中的第一列（T2 NB+PME）和第二列（T40 NB+PME），可以发现随着并行线程数的增加，主要耗时项目由Force（这是CPU计算BF的耗时）变成Wait GPU NB local（这是等待GPU计算NB的耗时）。<br>4）PME分配给CPU的情况下，计算速度在10个线程前快速上升，之后上升速度减慢，到30个线程时达到上限。根据表二中的第三列(T40 NB+BF)我们发现此时CPU计算PME的任务(PME mesh)是计算的瓶颈。<br>5）在不使用GPU的情况下，通过表一的最后第四列可以发现，PP的计算是最耗时的项目（Force），其次是PME的计算（PME mesh），但是计算PP的并行效率高，所以到40核的时候（第五列），PME反而成为最耗时的项目。</p>
<p><strong>b) tMPI多Rank，单显卡</strong></p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478614.php" alt="图二：多Rank并行下，计算速度随openmp线程数的变化情况。" style="zoom:50%;"></p>
<p><strong>分析：</strong></p>
<ul>
<li><p>单线程使用GPU计算NB+PME时，运行速度最快，此时CPU并行在10线程左右达到上限。显卡成为了运算的瓶颈，那么还剩下的34核能否进一步提高计算速度？</p>
<p>  在显卡成为瓶颈的情况下想要进一步提高运算速度，必须把显卡的PME任务分给CPU（上面说过NB任务不能单独分给CPU）。在上面单线程情况下，用CPU计算PME时，30核就基本达到速度上限，此时运算速度并不能超过GPU计算PME的速度。</p>
</li>
<li><p>那么使用多rank的情况下，能否提高CPU计算PME的并行效率呢？</p>
<p>  答案是否定的，经过我的测试，4 Rank，每Rank10线程和1 Rank，40线程的运行PME mesh的时间是基本相当的。</p>
<p>  不仅如此，多Rank情况下还会有额外的耗时，包括domain decomposition (DD)的耗时，多Rank共用一块显卡造成的效率下降，PP rank 和PME rank 之间的负载不平衡，DD 造成的负载不平衡。最后结果如图二所示，在一块显卡的情况下使用多Rank并行，并不能带来运算速度的提升。4*10=40核的运算速度反而没有1*10=10核的运算速度快。<br>  因此：<strong>单显卡使用多Rank时，多Rank对PME的运算效率不会提高，同时多Rank并行会带来一堆额外的耗时项目和负载不平衡，最终会带来速度的下降。</strong></p>
</li>
</ul>
<p><strong>c) tMPI 多Rank，双显卡</strong></p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478615.php" alt="图三：多Rank并行下，使用2块显卡的计算速度随openmp线程数的变化情况。" style="zoom:50%;"></p>
<p><strong>分析：</strong><br>1）多Rank下，不使用GPU计算PME的情况：  图三中的黄线和深蓝线，这是在运行gmx mdrun时不额外加“-pmp gpu -npme 1”时的默认结果。此时使用cpu计算pme，拿两块显卡去计算NB和BF，可以看到此时cpu计算PME成为了瓶颈，所以两块显卡算NB，反而还没有一块显卡同时计算NB+PME快。不仅如此，更为荒谬的是两块显卡算NB+BF时还没有一块显卡算NB+BF快（对比图三种的黄线最后一个点<50ns 和图一中黄线中间的点>70ns)。这应该是在使用显卡算NB后，CPU计算PME成为了瓶颈，而PME多rank并行的效率并没有提高，加速多Rank运行时各种负载不平衡带来的消耗。<br>2）多Rank下，使用GPU计算PME的情况：<br>  即使加上“-pmp gpu -npme 1”后，在2 Rank情况下使用1块显卡计算PME，1块显卡计算NB，速度竟然没有只使用1块显卡同时算PME和NB快。通过观察md.log中的结果可以发现，耗时项最大的是PME wait for PP <em>，大概意思就是一块显卡PME算好了，等另外一块显卡算NB等了半天。还可以看到log文件中“Average PME mesh/force load: 0.738”这样的描述，也就是PME/PP负载不平衡以及Rank之间通信造成了效率低下。在4 Rank下，<em>*拿出1个Rank做PME，以及3个Rank做PP后，PP/PME负载不平衡的情况情况得到了改善，此时两块显卡的运算速度也终于略微超过一块。</em></em><br>3）多Rank下GPU的任务分配：<br>  默认情况下Rank任务是平均分配给GPU的，比如这里我有4个Rank任务，2个GPU，那么平均一个GPU分到2个Rank，由于3个Rank是PP，一个Rank是PME，最后结果就是一个显卡计算2个PP任务，一个显卡计算1个PME+1个PP任务。此时可以用-gputasks 选项分配如何把这4个rank分给GPU，这4个rank中，前3个是PP rank，最后一个是PME rank，因此如果是”-gputasks 0001” 指定前三个PP rank分给0号gpu，最后一个PME rank分给1号gpu，而”-gputasks 0011”则相当于默认情况。本次测试中两种请</50ns></p>
<p><strong>d) N任务，N显卡</strong><br>上面的测试表明，两块显卡相对于一块显卡的提升非常有限。如果装了两块显卡，想有效的利用这两块显卡，最好的办法是每块显卡跑一个独立gmx任务。问题是，这两个独立的gmx会相互干扰么？经过测试，结论是：<strong>cpu核数足够的情况下两块显卡单独运行两个Gromacs任务完全没有影响</strong>。<a target="_blank" rel="noopener" href="http://bbs.keinsci.com/thread-13910-1-1.html?tdsourcetag=s_pcqq_aiomsg">JCC,2019</a>的那篇文章中也可以看到，N显卡相对于单显卡的速度几乎就是N倍。<br>    <strong>e) GTX1080的表现</strong></p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367939437841.php" alt="图四：2块1080显卡的测试情况，其中图标R2:NB+PME的意思是使用2个Rank，用GPU算NB和PME。"></p>
<p><strong>分析：</strong><br>1080和2080相比有下面一些相同和不同点<br>相同点：<br>1）单Rank下，显卡计算NB+PME时，CPU在10核计算速度达到饱和。<br>2）单显卡下，双Rank比单Rank慢<br>3）双显卡下，只有使用4Rank，且用显卡计算NB+PME时，速度才能略微超过单显卡情况<br>不同点：<br>1）单Rank下，随着openmp threads数量增多，显卡计算NB+BF的速度最终超过了NB+PME，这是因为1080性能略差，这样CPU并行数量上去之后，CPU计算PME的速度最终能够超过GPU计算PME的速度。</p>
<p><strong>f) 关于PME tuning</strong><br>PME部分的计算其实还有很多可以调控的地方，我没有深入研究，这里简单介绍一下：<br>1）tunepme，这是mdrun的选项，默认为开启，在CPU计算NB，GPU计算PME的情况下，为了使两边计算负载平衡，达到同步完成，gromacs采用了PME调控功能，其原理是增大rcoulomb，同时增加Fourierspacing，也就是减少FFT 格点数，使将使得更多的粒子长程作用从PME部分划分给NB部分。从而减少PME的计算量。所有我们在mdp文件中可以将rcoulomb和Fourierspacing 设小一点（比如0.8和1），让mdrun自行调控。</p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478617.php" alt="img"></p>
<p>2）vdwtype，这是mdp参数，如果设为pme，那么会使用PME计算长程VDW作用，不过如果这样做，无法使用GPU进行PME计算，这就相当于把更多的任务分配给了PME，因此，如果GPU很次，可以尝试这种做法。<br>3）pmefft，这是mdrun的选项，可以把PME的3D FFT单独分配给CPU算，而其他部分任然交给GPU算，据说用比较次的GPU搭配很好的CPU可以用这个选项，但是我尝试之后并没有得到积极的结果。<br>4）gmx tune_pme，这是gmx的一个程序，可以系统的优化PME参数，在给定总的Rank数情况下优化计算PME的rank数，以及rcolulomb和Fourierspacing的参数。比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx tune_pme -mdrun &#39;gmx mdrun -pme cpu&#39; -ntmpi 1 -ntomp 44 -rmax 2 -rmin 0 -ntpr 10 -gpu_id 0 -r 2 -fix 0 -s c05.tpr</span><br></pre></td></tr></table></figure>
<p>的意思是使用cpu算PME(-pme cpu),一共只使用1个tmpi rank(-ntmpi 1),对10个不同的rcoulomb设置进行测试(-ntpr 10)，其中rcoulomb最大值是2(-rmax 2), 最小值是tpr中的设定值(-rmin 0), 不使用独立的pme rank (-fix 0)，使用1个id为0的gpu加速计算(-gpu_id 0), 每个测试运行两遍(-r 2)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx tune_pme -mdrun &#39;gmx_mpi mdrun&#39; -np 20 -ntomp 2 -min 0.25 -max 0.5 -rmax 1.5 -rmin 0 -ntpr 5 -gpu_id 0 -r 2 -resetstep 3000 -steps 3000 -s c05.tpr</span><br></pre></td></tr></table></figure>
<p>使用20个openmpi rank(-np 20), 每个rank使用2个openmp thread, 其中PME 线程数从20*0.25=5个(-min 0.25) 测试到 20*0.5=10个 (-max 0.5)，每次测试运行3000步之后开始计时（-resetstep 3000), 计时时间为3000步(-steps 3000)。<br>该命令的更多选项见手册.</p>
<p><strong>g) cpu频率的影响</strong></p>
<p><img src="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/forum-16367926478618.php" alt="图五：单Rank，使用显卡跑NB+PME任务时，CPU频率对运算速度的影响。" style="zoom:50%;"></p>
<p><strong>分析：</strong><br>因为我们的结论是单Rank单显卡跑MD任务效率最高，那么剩下的问题就是CPU的频率以及核心数是如何影响运算速度的。<br>根据图五的结果我们可以做以下几点分析：<br>1）cpu高频时，能用更少的核的达到速度上限。<br>2）cpu低频时，10核之前并行效率高，10-20核并行效率低，20核之后并行效率几乎没有。<br>3）cpu频率的高低，会影响到速度上限的高低。</p>
<h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a><strong>结论：</strong></h2><p><strong>抽象版：
</strong>影响Gromacs效率的关键是下面3个平衡：<br>1，PME-NB运算任务之间的平衡。<br>1，CPU-GPU负载平衡。<br>2，多Rank并行时，Rank之间的负载平衡。<br>我们主要通过决定把多少资源（GPU，CPU，Rank）分配给PME和NB任务来实现上面的平衡。</p>
<p><strong>简单粗暴版：</strong><br><strong>单显卡情况下：</strong><br>只用1个Rank（运行时单进程多线程并行），如果显卡足够好，把PME任务给显卡，openmp theads 12个左右；命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx mdrun -pin on -ntmpi 1 -ntomp 12 -pme gpu XXX.tpr</span><br></pre></td></tr></table></figure>
<p>如果显卡较差，把PME任务给CPU，openmp theads 越多越好（一般超过20，计算速度达到上限），命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx mdrun -pin on -ntmpi 1 -ntomp 20 -pme cpu XXX.tpr</span><br></pre></td></tr></table></figure>
<p><strong>多显卡情况下：</strong><br>最好是给每个显卡一个Rank，单独跑一个Gromacs任务，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gmx mpirun -ntmpi 1 -ntomp 12 -gpu_id 0 -s abc.tpr #使用0号gpu计算abc.tpr</span><br><span class="line"></span><br><span class="line">gmx mpirun -ntmpi 1 -ntomp 12 -gpu_id 1 -s xyz.tpr #使用1号gpu计算abc.tpr</span><br></pre></td></tr></table></figure>
<p>如果非要用多个显卡跑一个MD任务，请把一个PME rank分配给显卡，其他的3个PP Rank分配给其他的显卡，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmx mdrun -pin on -ntmpi 4 -ntomp 8 -pme gpu -npme 1 XXX.tpr</span><br></pre></td></tr></table></figure>
<p><strong>给购机同学的建议：</strong><br>由于最高的效率的资源利用方式是单显卡跑任务，所以一个机器里面装多显卡的目的主要是为了同时跑多个gromacs任务（并且不占地方，便于操作），这时候需要考虑的是cpu核心数/gpu的比例，根据本次测试的结果，在GPU跑PME+NB，对于1080显卡，cpu 8核就能达到速度上限，对于 2080显卡，cpu 10 核就能达到上限，这说明越是好的显卡，也需要更多/更快的cpu核心数与之配套，才能充分发挥这块显卡的功能，另外根据第三部分g）CPU频率的测试可以发现，cpu10核之内thread并行效率高，10-20核并行效率低，20核之后并行效率几乎没有，因此<strong>如果只配一块显卡，我们有一个核心数较少（8-12）频率较高的CPU就够了，比如桌面级别的I7，I9，如果要装多显卡的机器，那么选择cpu的时候最好满足10核左右/1块显卡比例</strong>（如果cpu主频够高，可以适当降低核心数要求）<strong>，</strong>在此基础上，利用公式：cpu频率<em> min(显卡数</em>10，cpu核心数) / price，来计算搭配cpu的性价比。<br><strong><br>**</strong>给使用超算中心同学的建议：<strong>
</strong>对于slurm系统，你在用sbatch提交任务的时候，不管它的节点上有几块显卡 ，你每次提交任务的时候一律用从0开始，一次用一块显卡算就是-gpu id 0。一次用两块显卡算就是-gpu_id 01,** 这是因为大多数slurm系统都用cgroup管理资源。最大的资源的利用仍然是一次使用一块gpu，一个提交脚本申请一个gpu，提交一个任务即可，至于配合多少cpu，偷懒的话直接设成10，否者用下面一节测试脚本中的多GPU多任务情况的命令测试一下，需要多少CPU核数够。如果GPU节点上有多块显卡而又强制是exclusive模式（独占节点模式），那么应当在一个提交脚本中写入多个任务，或者使用多显卡算一个任务（4 Rank并行，PME分配给GPU）（待补充）</p>
<p><strong>五, 测试脚本</strong><br>最后附上本次测试用到的脚本：gmxbench.sh<br>安装之后（放到PATH路径下，加可执行权限），运行方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmxbench.sh -r &quot;1 2 4&quot; -T &quot;2 2 10&quot; -g &quot;0 01&quot; -G &quot;0 1 2 3 4&quot; -a &quot;-pin on&quot; -s 20000 -S 10000 XXX.tpr</span><br></pre></td></tr></table></figure>
<p>意思是，测试rank 为 1 2 4 时候的情况 (-r “1 2 4”)<br>对于每种rank设置，测试openmp 线程为“2 4 6 8 10”时候的情况(-T “2 2 10”)<br>对于上面每种设置，分别测试只使用0号GPU和同时使用01号GPU的情况(-g “0 01”)<br>对于上面每种设置，分别测试gpu分配任务为“0 1 2 3 4”时候的情况，这里的代号和第一部分中的“显卡加速的5种情况”中的编号对应(-G “0 1 2 3 4”)<br>每次测试运行20000步(-s 20000)，从10000步开始计时(-S 10000)，注意-S如果设置过小（比如小于5000，可能会因为还未完成pme tune 而出错）<br>最后额外添加关键词“-pin on” (-a “-pin on”)另外可以输入gmxbench.sh -h 查看帮助<br><strong>实用测试命令：</strong><br>根据我们的上面的结论，其实不用做那么多测试，下面是实用的测试命令<br><strong>单GPU情况：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmxbench.sh -r 1 -g 0 -G &quot;2 3&quot; -a &quot;-pin on&quot; -s 20000 -S 10000 XXX.tpr </span><br></pre></td></tr></table></figure>
<p>这里主要区别把PME分给CPU快还是GPU快，这里不设置openmp thread 会默认使用最大thread<br><strong>多GPU单任务情况：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmxbench.sh -r &quot;1 4&quot; -g &quot;0 01&quot; -G &quot;2 3&quot; -a &quot;-pin on&quot; -s 20000 -S 10000 XXX.tpr</span><br></pre></td></tr></table></figure>
<p>这里主要区别把PME分给CPU还是GPU以及是使用单Rank快还是4 Rank快，以及是用一块gpu快还是2块gpu快。。。这里不设置openmp thread 会默认使用最大thread<br><strong>多GPU多任务情况：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gmxbench.sh -r &quot;1&quot; -g &quot;0&quot; -G &quot;2&quot; -T &quot;6 2 20&quot; -a &quot;-pin on&quot; -s 20000 -S 10000 XXX.tpr</span><br></pre></td></tr></table></figure>
<p>这里主要是看使用一个GPU算一个任务时，用多少CPU能把GPU”喂饱“，因此我们固定其他参数，只扫描openmp线程，从6开始，每次增加2核，扫描到20（-T 2 1 20）<br><strong>测试完成后自动给出耗时统计，如果中途意外中断了可以重新运行相同的命令，会自动续算。
</strong><br>本人做动力学经验不多，希望各位动力学大佬提出宝贵建议和补充！！</p>
<h2 id="gmxbench源码"><a href="#gmxbench源码" class="headerlink" title="gmxbench源码"></a>gmxbench源码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;usage: -[c number of total cores] -[r rank list] -[t thread list] -[T thread in &quot;start step stop&quot; format] -[g gpu id list]  -[s total step] -[S reset step] -[G type of gpu task 0:no gpu, 1:nb, 2:nb+pme, 3:nb+bf, 4:nb+pme+bf]  -[a addition options for mdrun]   input.tpr&#x27;</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">getopts</span> <span class="string">&quot;:r:t:T:c:g:G:s:S:a:&quot;</span> opt</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">case</span> <span class="variable">$opt</span> <span class="keyword">in</span></span><br><span class="line">        r) RANKLIST=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        t) THREADLIST=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        T) THREADSEQ=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        c) CORES=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        g) GPUID=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        G) GPUTASK=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        s) TOTALSTEP=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        S) RESETSTEP=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        a) ADDOP=<span class="variable">$OPTARG</span>;;</span><br><span class="line">        ?) <span class="built_in">echo</span> <span class="string">&#x27;usage: -[c number of total cores] -[r rank list] -[t thread list] -[T thread in &quot;start step stop&quot; format] -[g gpu id list]  -[s total step] -[S reset step] -[G type of gpu task 0:no gpu, 1:nb, 2:nb+pme, 3:nb+bf, 4:nb+pme+bf]  -[a addition options for mdrun]   input.tpr&#x27;</span></span><br><span class="line">        <span class="built_in">exit</span> 1;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">shift</span> $((<span class="variable">$OPTIND</span> - <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">MDRUN=<span class="string">&quot;gmx mdrun&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="variable">$CORES</span> ];<span class="keyword">then</span></span><br><span class="line">    CORES=$(lscpu -p | egrep -v <span class="string">&#x27;^#&#x27;</span> | sort -u -t, -k 2,4 | wc -l)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$CORES</span> cores detected&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Total cores is <span class="variable">$CORES</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="variable">$RANKLIST</span> ];<span class="keyword">then</span></span><br><span class="line">    RANKLIST=<span class="string">&quot;1&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Rank list is <span class="variable">$RANKLIST</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$GPUID</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    ngpu=$(lspci | grep -c VGA)</span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$ngpu</span> -eq 1 ];<span class="keyword">then</span></span><br><span class="line">        GPUID=<span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$ngpu</span> -eq 2 ];<span class="keyword">then</span></span><br><span class="line">        GPUID=<span class="string">&quot;01 0&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$ngpu</span> -eq 3 ];<span class="keyword">then</span></span><br><span class="line">        GPUID=<span class="string">&quot;012 01 0&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$ngpu</span> -eq 4 ];<span class="keyword">then</span></span><br><span class="line">        GPUID=<span class="string">&quot;0123 012 01 0&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$ngpu</span> gpus detected&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;GPUID list is <span class="variable">$GPUID</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">&quot;<span class="variable">$THREADLIST</span>&quot;</span> &amp;&amp; -z <span class="string">&quot;<span class="variable">$THREADSEQ</span>&quot;</span> ]];<span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Use all the available cores only&quot;</span></span><br><span class="line">    ALLTHREAD=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$THREADSEQ</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    THREADLIST=$(seq <span class="variable">$THREADSEQ</span>)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Thread seq is <span class="variable">$THREADSEQ</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$THREALIST</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Thread list is <span class="variable">$THREADLIST</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$TOTALSTEP</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    TOTALSTEP=13000   <span class="comment"># Total steps to perform for each benchmark</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$RESETSTEP</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    RESETSTEP=8000   <span class="comment"># Total steps to perform for each benchmark</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$GPUTASK</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    GPUTASK=2</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Where is the benchmark MD system</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line">TPR=<span class="variable">$1</span></span><br><span class="line">TPRDIR=<span class="variable">$&#123;TPR/.tpr/&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># From the number of GPUs per node and the number of PP ranks</span></span><br><span class="line"><span class="comment"># determine an appropriate value for mdrun&#x27;s &quot;-gpu_id&quot; string.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># GPUs will be assigned to PP ranks in order, from the lower to</span></span><br><span class="line"><span class="comment"># the higher IDs, so that each GPU gets approximately the same</span></span><br><span class="line"><span class="comment"># number of PP ranks. Here is an example of how 5 PP ranks would</span></span><br><span class="line"><span class="comment"># be mapped to 2 GPUs:</span></span><br><span class="line"><span class="comment">#            +-----+-----+-----+-----+-----+</span></span><br><span class="line"><span class="comment"># PP ranks:  |  0  |  1  |  2  |  3  |  4  |</span></span><br><span class="line"><span class="comment">#            +-----+-----+-----+-----+-----+</span></span><br><span class="line"><span class="comment"># GPUs:      |  0  |  0  |  0  |  1  |  1  |</span></span><br><span class="line"><span class="comment">#            +-----+-----+-----+-----+-----+</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Will consecutively use GPU IDs from the list passed to this</span></span><br><span class="line"><span class="comment"># function as the third argument.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">func.getGpuString ( )</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$#</span> -ne 3 ]; <span class="keyword">then</span> </span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;ERROR: func.getGpuString needs #GPUs as 1st, #MPI as 2nd, and&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;       a string with allowed GPU IDs as 3rd argument (all per node)!&quot;</span> &gt;&amp;2</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;       It got: &#x27;<span class="variable">$@</span>&#x27;&quot;</span> &gt;&amp;2</span><br><span class="line">        <span class="built_in">exit</span> 333</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># number of GPUs per node:</span></span><br><span class="line">    <span class="built_in">local</span> NGPU=<span class="variable">$1</span></span><br><span class="line">    <span class="comment"># number of PP ranks per node:</span></span><br><span class="line">    <span class="built_in">local</span> N_PP=<span class="variable">$2</span></span><br><span class="line">    <span class="comment"># string with the allowed GPU IDs to use:</span></span><br><span class="line">    <span class="built_in">local</span> ALLOWED=<span class="variable">$3</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">local</span> currGPU=0</span><br><span class="line">    <span class="built_in">local</span> nextGPU=1</span><br><span class="line">    <span class="built_in">local</span> iPP</span><br><span class="line">    <span class="comment"># loop over all PP ranks on a node:</span></span><br><span class="line">    <span class="keyword">for</span> ((iPP=0; iPP &lt; <span class="variable">$N_PP</span>; iPP++)); <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">local</span> currGpuId=<span class="variable">$&#123;ALLOWED:$currGPU:1&#125;</span> <span class="comment"># single char starting at pos $currGPU</span></span><br><span class="line">        <span class="built_in">local</span> nextGpuId=<span class="variable">$&#123;ALLOWED:$nextGPU:1&#125;</span> <span class="comment"># single char starting at pos $nextGPU</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># append this GPU&#x27;s ID to the GPU string:</span></span><br><span class="line">        <span class="built_in">local</span> GPUSTRING=<span class="variable">$&#123;GPUSTRING&#125;</span><span class="variable">$&#123;currGpuId&#125;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check which GPU ID the _next_ MPI rank should use:</span></span><br><span class="line">        <span class="built_in">local</span> NUM=$( <span class="built_in">echo</span> <span class="string">&quot;(<span class="variable">$iPP</span> + 1) * <span class="variable">$NGPU</span> / <span class="variable">$N_PP</span>&quot;</span> | bc -l )</span><br><span class="line">        <span class="built_in">local</span> COND=$( <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$NUM</span> &gt;= <span class="variable">$nextGPU</span>&quot;</span> | bc )</span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$COND</span>&quot;</span> -eq <span class="string">&quot;1&quot;</span> ] ; <span class="keyword">then</span></span><br><span class="line">            ((currGPU++))</span><br><span class="line">            ((nextGPU++))</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># return the constructed string:</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># Do the benchmarks!</span></span><br><span class="line"><span class="comment">#==============================================================================</span></span><br><span class="line"><span class="comment"># 1 generate DIR</span></span><br><span class="line">DIR=$( <span class="built_in">pwd</span> )</span><br><span class="line">DIRCOUNT=0</span><br><span class="line">mkdir <span class="string">&quot;<span class="variable">$DIR</span>&quot;</span>/<span class="variable">$TPRDIR</span></span><br><span class="line">cp <span class="variable">$TPR</span>  <span class="string">&quot;<span class="variable">$DIR</span>&quot;</span>/<span class="variable">$TPRDIR</span></span><br><span class="line"><span class="keyword">for</span> NTMPI <span class="keyword">in</span> <span class="variable">$RANKLIST</span> ; <span class="keyword">do</span>  <span class="comment"># list with all numbers of thread-MPI ranks to use</span></span><br><span class="line">    NTOMPMAX=$( <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$CORES</span> / <span class="variable">$NTMPI</span>&quot;</span> | bc )</span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$ALLTHREAD</span> -eq 1 ]];<span class="keyword">then</span></span><br><span class="line">        THREADLIST=<span class="variable">$NTOMPMAX</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="comment">#for NGPU in $NGPU_PER_HOST; do</span></span><br><span class="line">        <span class="comment">#GPUSTR=$( func.getGpuString $NGPU $NTMPI $USEGPUIDS )</span></span><br><span class="line">    <span class="keyword">for</span> GPUSTR <span class="keyword">in</span> <span class="variable">$GPUID</span>; <span class="keyword">do</span></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$NTMPI</span>&quot;</span> -eq 1 &amp;&amp; <span class="string">&quot;<span class="variable">$&#123;#GPUSTR&#125;</span>&quot;</span> -gt 1 ]];<span class="keyword">then</span></span><br><span class="line">            <span class="built_in">continue</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="string">&quot;<span class="variable">$GPUSTR</span>&quot;</span> == <span class="string">&quot;None&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">            GPUCOM=<span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            GPUCOM=<span class="string">&quot;-gpu_id <span class="variable">$GPUSTR</span>&quot;</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">for</span> gt <span class="keyword">in</span> <span class="variable">$GPUTASK</span>; <span class="keyword">do</span></span><br><span class="line">            <span class="keyword">case</span> <span class="variable">$gt</span> <span class="keyword">in</span></span><br><span class="line">                0) GPUFLAG=<span class="string">&#x27;-nb cpu -pme cpu -bonded cpu&#x27;</span>;;</span><br><span class="line">                1) GPUFLAG=<span class="string">&#x27;-nb gpu -pme cpu -bonded cpu&#x27;</span>;;</span><br><span class="line">                2) <span class="keyword">if</span> [ <span class="variable">$NTMPI</span> -eq 1 ];<span class="keyword">then</span> GPUFLAG=<span class="string">&#x27;-nb gpu -pme gpu -bonded cpu&#x27;</span>; <span class="keyword">else</span> GPUFLAG=<span class="string">&#x27;-nb gpu -pme gpu -bonded cpu -npme 1&#x27;</span>; <span class="keyword">fi</span>;;</span><br><span class="line">                3) GPUFLAG=<span class="string">&#x27;-nb gpu -pme cpu -bonded gpu&#x27;</span>;;</span><br><span class="line">                4) <span class="keyword">if</span> [ <span class="variable">$NTMPI</span> -eq 1 ];<span class="keyword">then</span> GPUFLAG=<span class="string">&#x27;-nb gpu -pme gpu -bonded gpu&#x27;</span>; <span class="keyword">else</span> GPUFLAG=<span class="string">&#x27;-nb gpu -pme gpu -bonded gpu -npme 1&#x27;</span>; <span class="keyword">fi</span>;;</span><br><span class="line">            <span class="keyword">esac</span></span><br><span class="line">            <span class="keyword">for</span> NTOMP <span class="keyword">in</span> <span class="variable">$THREADLIST</span>; <span class="keyword">do</span></span><br><span class="line">                <span class="keyword">if</span> [[ <span class="variable">$NTOMP</span> -le <span class="variable">$NTOMPMAX</span> &amp;&amp; <span class="variable">$NTOMP</span> -le 64 ]];<span class="keyword">then</span></span><br><span class="line">                    MPI=$(<span class="built_in">printf</span> <span class="string">&quot;%02d&quot;</span> <span class="variable">$NTMPI</span>)</span><br><span class="line">                    OMP=$(<span class="built_in">printf</span> <span class="string">&quot;%02d&quot;</span> <span class="variable">$NTOMP</span>)</span><br><span class="line">                    RUN=ntmpi<span class="variable">$&#123;MPI&#125;</span>_ntomp<span class="variable">$&#123;OMP&#125;</span>_gpuid<span class="variable">$&#123;GPUSTR&#125;</span>_gt<span class="variable">$&#123;gt&#125;</span></span><br><span class="line">                    mkdir <span class="string">&quot;<span class="variable">$DIR</span>&quot;</span>/<span class="variable">$TPRDIR</span>/run<span class="variable">$RUN</span></span><br><span class="line">                    <span class="built_in">cd</span> <span class="string">&quot;<span class="variable">$DIR</span>&quot;</span>/<span class="variable">$TPRDIR</span>/run<span class="variable">$RUN</span></span><br><span class="line">                    <span class="built_in">export</span> GMX_NSTLIST=20</span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">&quot; <span class="variable">$MDRUN</span> -ntmpi <span class="variable">$NTMPI</span> -ntomp <span class="variable">$NTOMP</span>  -s &quot;</span><span class="variable">$DIR</span>/<span class="variable">$TPRDIR</span>/<span class="variable">$TPR</span><span class="string">&quot; -cpt 1440 -nsteps <span class="variable">$TOTALSTEP</span> -resetstep <span class="variable">$RESETSTEP</span> -v -noconfout <span class="variable">$GPUCOM</span> <span class="variable">$GPUFLAG</span> <span class="variable">$ADDOP</span>&quot;</span> &gt; run.sh</span><br><span class="line">                    ((DIRCOUNT++))</span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">done</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$DIRCOUNT</span> test file generate completed&quot;</span></span><br><span class="line"><span class="comment"># check and run benchmark</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$DIR</span>&quot;</span>/<span class="variable">$TPRDIR</span>/run*;<span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$i</span></span><br><span class="line">    <span class="built_in">cd</span> <span class="variable">$i</span></span><br><span class="line">    flag=$(grep Finished md.log)</span><br><span class="line">    <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$flag</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">        sh run.sh</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="variable">$i</span><span class="string">&quot; finished&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment">#extract and analysis</span></span><br><span class="line"><span class="built_in">printf</span> <span class="string">&quot;%4s %4s %4s %8s %8s %9s %9s %9s %9s %9s\n&quot;</span> <span class="string">&quot;#GPU&quot;</span> <span class="string">&quot;#MPI&quot;</span> <span class="string">&quot;#OMP&quot;</span> <span class="string">&quot;GPUTASK&quot;</span> <span class="string">&quot;ns/day&quot;</span> <span class="string">&quot;WaitGPU_PME&quot;</span> <span class="string">&quot;PME_MESH&quot;</span> <span class="string">&quot;WaitGPU_NB&quot;</span> <span class="string">&quot;Force&quot;</span> <span class="string">&quot;Constraints&quot;</span> | tee <span class="variable">$DIR</span>/<span class="variable">$TPRDIR</span>-results.log</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$DIR</span>&quot;</span>/<span class="variable">$TPRDIR</span>/run*;<span class="keyword">do</span></span><br><span class="line">   <span class="built_in">cd</span> <span class="variable">$i</span></span><br><span class="line">   GTID=<span class="string">&quot;<span class="variable">$&#123;i: -1&#125;</span>&quot;</span></span><br><span class="line">   FILENM=md.part0001.log</span><br><span class="line">   <span class="keyword">if</span> [ ! -f <span class="string">&quot;<span class="variable">$FILENM</span>&quot;</span> ] ; <span class="keyword">then</span></span><br><span class="line">       FILENM=md.log</span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">   VERSION=`grep      <span class="string">&#x27;Gromacs version:&#x27;</span>             <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; print $4 &#125;&#x27;</span>`</span><br><span class="line">   CPUTYPE=`grep      <span class="string">&#x27;Brand:  &#x27;</span>                     <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; print $2 &quot; &quot; $5 &#125;&#x27;</span>`</span><br><span class="line">   GPUTYPE=`grep      <span class="string">&#x27; #0: NVIDIA &#x27;</span>                 <span class="variable">$FILENM</span> | cut -f 1 -d , |  awk <span class="string">&#x27;&#123; print $5 $6 &#125;&#x27;</span>`</span><br><span class="line">   N_MPI__=`grep      <span class="string">&#x27; MPI thread&#x27;</span>                  <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; print $2 &#125;&#x27;</span>`</span><br><span class="line">   <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$N_MPI__</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">       N_MPI__=`grep <span class="string">&#x27;MPI processes&#x27;</span>                 <span class="variable">$FILENM</span> | grep <span class="string">&quot;Using&quot;</span> | awk <span class="string">&#x27;&#123; print $2 &#125;&#x27;</span>`</span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line">   N_OMP__=`grep      <span class="string">&#x27;Using [0-9]* OpenMP threads &#x27;</span>              <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; print $2 &#125;&#x27;</span>`</span><br><span class="line">   <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$N_OMP__</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">       N_OMP__=1</span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line">   N_GPU__=`grep      <span class="string">&#x27;-selected for this run&#x27;</span>       <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123;if ($1==&quot;On&quot;) print $4;else print $1 &#125;&#x27;</span>`</span><br><span class="line">   NANOSPD=`grep      <span class="string">&#x27;Performance&#x27;</span>                  <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; print $2 &#125;&#x27;</span>`</span><br><span class="line"><span class="comment">#   NEIGHBS=`grep      &#x27;Neighbor search&#x27;              $FILENM | awk &#x27;&#123; print $8 &#125;&#x27;`</span></span><br><span class="line"><span class="comment">#   NSTLIST=`grep      &#x27;nstlist  &#x27;                    $FILENM | awk &#x27;&#123; print $3 &#125;&#x27;`</span></span><br><span class="line"><span class="comment">#   ACCELER=`grep      &#x27;Acceleration most likely&#x27;     $FILENM | awk &#x27;&#123; print $8 &#125;&#x27;`</span></span><br><span class="line"><span class="comment">#   BRAND__=`grep      &#x27;Brand: &#x27;                      $FILENM | awk &#x27;&#123; print $3$5 &#125;&#x27;`</span></span><br><span class="line"><span class="comment">#   PMENODE=`grep      &#x27;, separate PME nodes&#x27;         $FILENM | awk &#x27;&#123; print $NF &#125;&#x27;`</span></span><br><span class="line"><span class="comment">#                   R_COUL_=`grep      &#x27;   optimal pme grid &#x27;         $FILENM | awk &#x27;&#123; print $9 &#125;&#x27;`</span></span><br><span class="line"><span class="comment">#   R_COUL_=`grep      &#x27;   final   &#x27;                  $FILENM | awk &#x27;&#123; print $2 &#125;&#x27;`</span></span><br><span class="line">   WG_PME_=`grep      <span class="string">&#x27;Wait PME GPU gather&#x27;</span>          <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; printf &quot;%6.2f(%3.1f)&quot;,$8,$10 &#125;&#x27;</span>`</span><br><span class="line">   <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$WG_PME_</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">       WG_PME_=NaN</span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line">   WG_NB__=`grep      <span class="string">&#x27;Wait GPU NB local&#x27;</span>            <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; printf &quot;%6.2f(%3.1f)&quot;,$8,$10 &#125;&#x27;</span>`</span><br><span class="line">   <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$WG_NB__</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">       WG_NB__=NaN</span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line">   FORCE__=`grep      <span class="string">&#x27; Force &#x27;</span>                      <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; printf &quot;%6.2f(%3.1f)&quot;,$5,$7 &#125;&#x27;</span>`</span><br><span class="line">   CONSTRS=`grep      <span class="string">&#x27; Constraints &#x27;</span>                <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; printf &quot;%6.2f(%3.1f)&quot;,$5,$7 &#125;&#x27;</span>`</span><br><span class="line">   PME_MESH=`grep     <span class="string">&#x27; PME mesh   &#x27;</span>                   <span class="variable">$FILENM</span> | awk <span class="string">&#x27;&#123; printf &quot;%6.2f(%3.1f)&quot;,$6,$8 &#125;&#x27;</span>`</span><br><span class="line">   <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$PME_MESH</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">       PME_MESH=NaN</span><br><span class="line">   <span class="keyword">fi</span></span><br><span class="line">   <span class="comment">#DOMDEC_=`grep      &#x27;Domain decomposition grid &#x27;   $FILENM | cut -d , -f 1 | awk &#x27;&#123; print $4 &quot; &quot; $6 &quot; &quot; $8 &#125;&#x27;`</span></span><br><span class="line">   <span class="comment">#if [ &quot;$N_MPI__&quot; == &quot;1&quot; ] ; then</span></span><br><span class="line">   <span class="comment">#    DOMDEC_=&quot;1 1 1&quot;</span></span><br><span class="line">   <span class="comment">#fi</span></span><br><span class="line">    <span class="built_in">printf</span> <span class="string">&quot;%4d %4d %4d %8d %8.3f %9s %9s %9s %9s %9s\n&quot;</span> <span class="string">&quot;<span class="variable">$&#123;N_GPU__&#125;</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;N_MPI__&#125;</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;N_OMP__&#125;</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;GTID&#125;</span>&quot;</span> <span class="string">&quot;<span class="variable">$NANOSPD</span>&quot;</span> <span class="string">&quot;<span class="variable">$WG_PME_</span>&quot;</span> <span class="string">&quot;<span class="variable">$PME_MESH</span>&quot;</span> <span class="string">&quot;<span class="variable">$WG_NB__</span>&quot;</span> <span class="string">&quot;<span class="variable">$FORCE__</span>&quot;</span> <span class="string">&quot;<span class="variable">$CONSTRS</span>&quot;</span> | tee -a <span class="variable">$DIR</span>/<span class="variable">$TPRDIR</span>-results.log</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fy-han.github.io.git/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/" data-id="ckw6adoup00404gu5ewp57sam" data-title="GROMACS 并行效率测试与调试" class="article-share-link">Compartir</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/gmx/" rel="tag">gmx</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2021/11/15/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-Pymol-2021-11-15-pymol%E6%95%99%E7%A8%8B/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Nuevo</strong>
      <div class="article-nav-title">
        
          pymol教程
        
      </div>
    </a>
  
  
    <a href="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%8D%9A%E5%AE%A2-2021-3-10-hexo-github%E5%BB%BA%E7%AB%8B%E5%8D%9A%E5%AE%A2%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98-2021-11-13-hexo%E5%9B%BE%E7%89%87%E9%85%8D%E7%BD%AE/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Viejo</strong>
      <div class="article-nav-title">hexo图片配置与hexo主题</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AMBER/" rel="tag">AMBER</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gaussian/" rel="tag">Gaussian</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ai/" rel="tag">ai</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gmx/" rel="tag">gmx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/moe/" rel="tag">moe</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pymol/" rel="tag">pymol</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/summer-intern/" rel="tag">summer intern</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web/" rel="tag">web</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BD%9C%E6%9B%B2/" rel="tag">作曲</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%8B/" rel="tag">博客建立</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%91%A8%E6%8A%A5/" rel="tag">周报</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/" rel="tag">技术杂谈</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%94%9F%E6%B4%BB%E5%8F%8D%E6%80%9D/" rel="tag">生活反思</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Nube de Tags</h3>
    <div class="widget tagcloud">
      <a href="/tags/AMBER/" style="font-size: 16px;">AMBER</a> <a href="/tags/Gaussian/" style="font-size: 10px;">Gaussian</a> <a href="/tags/ai/" style="font-size: 12px;">ai</a> <a href="/tags/gmx/" style="font-size: 18px;">gmx</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/moe/" style="font-size: 12px;">moe</a> <a href="/tags/pymol/" style="font-size: 10px;">pymol</a> <a href="/tags/summer-intern/" style="font-size: 10px;">summer intern</a> <a href="/tags/web/" style="font-size: 10px;">web</a> <a href="/tags/%E4%BD%9C%E6%9B%B2/" style="font-size: 10px;">作曲</a> <a href="/tags/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%8B/" style="font-size: 14px;">博客建立</a> <a href="/tags/%E5%91%A8%E6%8A%A5/" style="font-size: 20px;">周报</a> <a href="/tags/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/" style="font-size: 14px;">技术杂谈</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E5%8F%8D%E6%80%9D/" style="font-size: 14px;">生活反思</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archivos</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Posts recientes</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2021/11/15/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-Pymol-2021-11-15-pymol%E6%95%99%E7%A8%8B/">pymol教程</a>
          </li>
        
          <li>
            <a href="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-%E5%B9%B6%E8%A1%8C%E6%95%88%E7%8E%87%E6%B5%8B%E8%AF%95%E4%B8%8E%E8%B0%83%E8%AF%95/">GROMACS 并行效率测试与调试</a>
          </li>
        
          <li>
            <a href="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%8D%9A%E5%AE%A2-2021-3-10-hexo-github%E5%BB%BA%E7%AB%8B%E5%8D%9A%E5%AE%A2%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98-2021-11-13-hexo%E5%9B%BE%E7%89%87%E9%85%8D%E7%BD%AE/">hexo图片配置与hexo主题</a>
          </li>
        
          <li>
            <a href="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F-GMX-2021-11-13-GROMACS-PCA/">GROMACS PCA</a>
          </li>
        
          <li>
            <a href="/blog/2021/11/13/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%8A%80%E6%9C%AF-%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88-2021-11-13-shell%E8%84%9A%E6%9C%AC%E6%84%8F%E6%80%9D-2021-11-13-shell%E8%84%9A%E6%9C%AC%E6%84%8F%E6%80%9D/">shell脚本意思</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 FY-Han<br>
      Construido por <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnsjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>